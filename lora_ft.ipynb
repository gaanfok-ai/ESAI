{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdb5ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaanfok/anaconda3/envs/thinkslm/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import pynvml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591377ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32156ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4957, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openbookqa_train = load_dataset(\"allenai/openbookqa\", \"main\", split=\"train\")\n",
    "openbookqa_validation = load_dataset(\"allenai/openbookqa\", \"main\", split=\"validation\")\n",
    "openbookqa_test = load_dataset(\"allenai/openbookqa\", \"main\", split=\"test\")\n",
    "\n",
    "arc_easy_test = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
    "arc_chal_test = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "\n",
    "len(openbookqa_train), len(openbookqa_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b505f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f2512a95b149688f54ea3e64b6cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "if device == \"cpu\":\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb6ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07444d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANON = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def obqa_prompt_answer(ex):\n",
    "    labels = list(ex[\"choices\"][\"label\"])\n",
    "    choices = list(ex[\"choices\"][\"text\"])\n",
    "    answer = ex[\"answerKey\"]\n",
    "\n",
    "    mapping = {orig: CANON[i] for i, orig in enumerate(labels)}\n",
    "    opts = \"\\n\".join([f\"{mapping[l]}. {c}\" for l, c in zip(labels, choices)])\n",
    "\n",
    "    prompt = (\n",
    "        \"Answer the multiple-choice question.\\n\"\n",
    "        \"Reply with exactly one line in this format:\\n\"\n",
    "        \"Final answer: <LETTER>\\n\\n\"\n",
    "        f\"Question: {ex['question_stem']}\\n\"\n",
    "        f\"Choices:\\n{opts}\\n\"\n",
    "        \"\\n\"  # model should answer next\n",
    "    )\n",
    "    completion = f\"Final answer: {mapping[answer]}\"\n",
    "\n",
    "    return {\"prompt\": prompt, \"completion\": completion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e637afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_masked(batch, max_length=512):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    completions = batch[\"completion\"]\n",
    "\n",
    "    input_ids_list = []\n",
    "    attn_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for p, c in zip(prompts, completions):\n",
    "        # tokenize separately (no special tokens inserted twice)\n",
    "        p_ids = tokenizer(p, add_special_tokens=False).input_ids\n",
    "        c_ids = tokenizer(c, add_special_tokens=False).input_ids\n",
    "\n",
    "        # add EOS to completion so model learns to stop\n",
    "        c_ids = c_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "        input_ids = p_ids + c_ids\n",
    "        if len(input_ids) > max_length:\n",
    "            # truncate from the left of the prompt (keep completion)\n",
    "            # ensure we don't truncate completion away\n",
    "            keep = max_length\n",
    "            input_ids = input_ids[-keep:]\n",
    "            # recompute boundary approximately: we will mask everything\n",
    "            # except last len(c_ids) tokens (safe if completion retained)\n",
    "            c_len = len(c_ids)\n",
    "            labels = [-100] * (len(input_ids) - c_len) + input_ids[-c_len:]\n",
    "        else:\n",
    "            labels = [-100] * len(p_ids) + c_ids\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attn_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\"input_ids\": input_ids_list, \"attention_mask\": attn_list, \"labels\": labels_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654b036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa = openbookqa_train.map(obqa_prompt_answer, remove_columns=openbookqa_train.column_names)\n",
    "val_pa   = openbookqa_validation.map(obqa_prompt_answer, remove_columns=openbookqa_validation.column_names)\n",
    "\n",
    "train_tok = train_pa.map(lambda b: tokenize_masked(b, max_length=256), batched=True, remove_columns=train_pa.column_names)\n",
    "val_tok   = val_pa.map(lambda b: tokenize_masked(b, max_length=256), batched=True, remove_columns=val_pa.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cbc85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 4957\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1663ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [16141,\n",
       "  279,\n",
       "  5248,\n",
       "  62626,\n",
       "  3405,\n",
       "  624,\n",
       "  20841,\n",
       "  448,\n",
       "  6896,\n",
       "  825,\n",
       "  1555,\n",
       "  304,\n",
       "  419,\n",
       "  3561,\n",
       "  510,\n",
       "  19357,\n",
       "  4226,\n",
       "  25,\n",
       "  366,\n",
       "  20756,\n",
       "  4198,\n",
       "  1339,\n",
       "  14582,\n",
       "  25,\n",
       "  576,\n",
       "  7015,\n",
       "  374,\n",
       "  8480,\n",
       "  369,\n",
       "  198,\n",
       "  89283,\n",
       "  510,\n",
       "  32,\n",
       "  13,\n",
       "  56625,\n",
       "  6832,\n",
       "  501,\n",
       "  28762,\n",
       "  198,\n",
       "  33,\n",
       "  13,\n",
       "  2841,\n",
       "  7826,\n",
       "  705,\n",
       "  323,\n",
       "  3709,\n",
       "  2310,\n",
       "  198,\n",
       "  34,\n",
       "  13,\n",
       "  19281,\n",
       "  30231,\n",
       "  1280,\n",
       "  304,\n",
       "  264,\n",
       "  92384,\n",
       "  198,\n",
       "  35,\n",
       "  13,\n",
       "  10779,\n",
       "  8151,\n",
       "  10909,\n",
       "  11,\n",
       "  14211,\n",
       "  17765,\n",
       "  323,\n",
       "  30231,\n",
       "  1280,\n",
       "  271,\n",
       "  19357,\n",
       "  4226,\n",
       "  25,\n",
       "  422,\n",
       "  151645],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  19357,\n",
       "  4226,\n",
       "  25,\n",
       "  422,\n",
       "  151645]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a535b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the multiple-choice question.\n",
      "Reply with exactly one line in this format:\n",
      "Final answer: <LETTER>\n",
      "\n",
      "Question: The sun is responsible for\n",
      "Choices:\n",
      "A. puppies learning new tricks\n",
      "B. children growing up and getting old\n",
      "C. flowers wilting in a vase\n",
      "D. plants sprouting, blooming and wilting\n",
      "\n",
      "Final answer: D<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "decoded_input = tokenizer.decode(\n",
    "    train_tok[0][\"input_ids\"],\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "print(decoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42da06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CausalLMDataCollator:\n",
    "    tokenizer: Any\n",
    "    pad_to_multiple_of: int | None = None\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # Pad inputs using tokenizer utilities\n",
    "        batch_inputs = self.tokenizer.pad(\n",
    "            [{\"input_ids\": f[\"input_ids\"], \"attention_mask\": f[\"attention_mask\"]} for f in features],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "        )\n",
    "\n",
    "        max_len = batch_inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Pad labels with -100\n",
    "        labels = []\n",
    "        for f in features:\n",
    "            lab = f[\"labels\"]\n",
    "            labels.append(lab + [-100] * (max_len - len(lab)))\n",
    "\n",
    "        batch_inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "        return batch_inputs\n",
    "\n",
    "data_collator = CausalLMDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8 if device == \"cuda\" else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908991f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 745,472 || all params: 1,544,459,776 || trainable%: 0.0483\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b1b5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen25_obqa_lora_1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    group_by_length=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b57d9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pynvml.nvmlInit()\n",
    "HANDLE = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "def sample_power_w():\n",
    "    return pynvml.nvmlDeviceGetPowerUsage(HANDLE) / 1000.0\n",
    "\n",
    "class PowerCallback(TrainerCallback):\n",
    "    def __init__(self, handle, every_steps=10):\n",
    "        self.handle = handle\n",
    "        self.every_steps = every_steps\n",
    "        self.powers = []\n",
    "        self.times = []\n",
    "        self.t0 = None\n",
    "        self.t1 = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.t0 = time.time()\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.every_steps == 0:\n",
    "            self.powers.append(pynvml.nvmlDeviceGetPowerUsage(self.handle) / 1000.0)\n",
    "            self.times.append(time.time())\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        self.t1 = time.time()\n",
    "\n",
    "def summarize_training_power(cb: PowerCallback):\n",
    "    duration = (cb.t1 - cb.t0) if (cb.t0 is not None and cb.t1 is not None) else None\n",
    "    if not cb.powers:\n",
    "        return {\"train_time_sec\": duration, \"avg_power_w\": None, \"energy_j\": None}\n",
    "\n",
    "    avg_power = sum(cb.powers) / len(cb.powers)\n",
    "    # energy approximation across sampled interval\n",
    "    sampled_duration = (cb.times[-1] - cb.times[0]) if len(cb.times) >= 2 else duration\n",
    "    energy_j = avg_power * sampled_duration if sampled_duration is not None else None\n",
    "\n",
    "    return {\"train_time_sec\": duration, \"avg_power_w\": avg_power, \"energy_j\": energy_j}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb321f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/home/gaanfok/anaconda3/envs/thinkslm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1550' max='1550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1550/1550 1:38:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.083583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.145839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.125434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.114912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.101968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.095368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.080197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.073164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.080665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.079707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.069072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.075641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.059967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.042949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.043844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.044869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.049943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.055411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.034446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.020292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.034614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.035959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.022810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.018953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.024267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.018942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.020237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.021052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.013013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.009241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.010867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaanfok/anaconda3/envs/thinkslm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gaanfok/anaconda3/envs/thinkslm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gaanfok/anaconda3/envs/thinkslm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gaanfok/anaconda3/envs/thinkslm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TrainOutput(global_step=1550, training_loss=0.08732984756269763, metrics={'train_runtime': 5904.3318, 'train_samples_per_second': 4.198, 'train_steps_per_second': 0.263, 'total_flos': 1.39507949862912e+16, 'train_loss': 0.08732984756269763, 'epoch': 5.0}),\n",
       " {'train_time_sec': 5904.330672979355,\n",
       "  'avg_power_w': 54.62311612903226,\n",
       "  'energy_j': 320232.88870997267})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_cb = PowerCallback(HANDLE, every_steps=10)\n",
    "trainer.add_callback(power_cb)\n",
    "\n",
    "train_out = trainer.train()\n",
    "train_metrics = summarize_training_power(power_cb)\n",
    "\n",
    "train_out, train_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinkslm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
